# RNN-encoder-decoder-attention_NLP_CS6320_UTD

CS 6320.002: Natural Language Processing
Fall 2020
Homework 5 --- 55 points

Issued 26 Oct. 2020
Due 11:59pm CST 09 Nov. 2020

Deliverables: The completed jupyter notebook and your saved model parameters hw5.encoder and hw5.decoder.

[Getting Started]

We are going to implement a sequence-to-sequence model, specifically an encoder-decoder with attention, using recurrent neural networks.

We will be using PyTorch again. In the last assignment, we implemented the neural network from scratch to familiarize ourselves with the bones of how they work. This time, we will be using some of the prebuilt layers in the torch.nn module for convenience.

We are using the E2E Challenge dataset for generating restuarant descriptions from meaning representations

ALL INSTRUCTIONS AND PROBLEM DEFINITION PRESENT IN THE NOTEBOOK (Original notebook has only question, solution present in HW5.ipynb)
